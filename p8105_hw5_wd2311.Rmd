---
title: "p8105_hw5_wd2311"
author: "wd2311"
date: "2025-11-14"
output: github_document
---
## Loading package
```{r}
library(tidyverse)
library(rvest)
```

## Figures settings
```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


# Problem 1

```{r}
set.seed(1)
```

## Define birthday-checking function
```{r}
bday_sim = function(n_room) {
  birthdays = sample(1:365, n_room, replace = TRUE)
repeated_bday = length(unique(birthdays)) < n_room
repeated_bday
}

bday_sim(20)
```

## 10000 simulation runs
```{r}
bday_sim_results = 
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(bdays) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

## Plot the function
```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    x = "Group Size", 
    y = "Probability",
    title = "Probability of Sharing Birthday for Increasing Group Size"
  )
```

The probability is very small for small groups, but it rises quickly as group size increases.
Around 23 people, the probability is already about 0.5â€”thereâ€™s roughly a 50% chance that at least two people share a birthday.By about 40â€“50 people, the probability is above 90%, approaching 1 as the group grows.
This is the classic birthday paradox. Even though there are 365 possible birthdays, we donâ€™t need anything close to 365 people before a match becomes very likely. The curveâ€™s steep, nonlinear rise shows how quickly shared birthdays become almost guaranteed as the group size increases.


## Problem 2

## Set ðœ‡=0 and Generate 5000 datasets from the model
```{r}
powermu_zero = 
  expand_grid(
    sample_n = 30,
    sample_sd = 5,
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(data = map(iter, ~rnorm(sample_n, mean = mu, sd = sample_sd))) |> 
  mutate(tidy = map(data, ~broom::tidy(t.test(.x, mu = 0)))) |> 
  mutate(mu_hat = map_dbl(tidy, "estimate"),
         p_value = map_dbl(tidy, "p.value"))

```


## Repeat the above for ðœ‡={1,2,3,4,5,6}
```{r}
power_sim = 
  expand_grid(
    mu = 1:6, 
    iter = 1:5000) |> 
  mutate(
    data = map(mu, ~ rnorm(n = 30, mean = .x, sd = 5)),
    ttest = map(data, ~broom::tidy(t.test(.x, mu = 0)))
    
  ) |> 
  unnest(ttest) |> 
  select(mu, iter, estimate, p.value)
  
```

## Describe the association between effect size and power.
```{r}
power_sim |> 
  group_by(mu) |> 
  summarize(
    power = mean(p.value < 0.05)
  ) |> 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
  x = "True Value of Î¼",
  y = "Power (Proportion Null Rejected)",
  title = "Power and the True Value of Î¼"
)
  
```

As the true value of Î¼ increases, the power of the one-sample t-test increases sharply. When Î¼ is near zero, the probability of rejecting the null hypothesis is near the nominal Î± = 0.05. As Î¼ grows from 1 to 3, the test becomes increasingly sensitive and power rises rapidly. For Î¼ â‰¥ 4, power approaches 1, meaning the test almost always detects the effect. This demonstrates that larger effect sizes lead to higher power, given fixed sample size and variance.


## Average Estimates of Î¼ vs. True Î¼
```{r}
power_sim |> 
  group_by(mu) |> 
  summarize(
    avg_est = mean(estimate),
    avg_est_rejectnull = mean(estimate[p.value < 0.05], na.rm = TRUE)
  ) |> 
  pivot_longer(cols = c(avg_est, avg_est_rejectnull),
               names_to = "type",
               values_to = "value") |> 
  ggplot(aes(x = mu, y = value, color = type)) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(labels = c("All Estimates", "Estimates (Null Rejected)")) +
  labs(
    x = "True Value of Î¼",
    y = "Average Estimate of Î¼",
    color = "Estimate Type",
    title = "Average Estimates of Î¼ vs. True Î¼"
  )
```

The average Î¼Ì‚ among samples where the null is rejected is not equal to the true Î¼,
especially when Î¼ is small.This is because restricting to significant tests selects 
only the samples in which random noise pushed Î¼Ì‚ upward enough to pass the significance
threshold. This creates upward bias in the estimate, known as the â€œwinnerâ€™s curseâ€ 
or significance filter bias. Only when Î¼ is large and power is high do the two 
averages converge and become approximately equal.




# Problem 3

## Setting up Data
```{r} 
homicide_df = read_csv(file = "./data/homicide-data.csv")
```
## Describe raw data
The dataset collected by The Washington Post contains detailed information on 
more than 52179 homicide cases from 50 major U.S. cities between 2007 and 2017. 
Each row represents a single homicide.

Key Variables: 
- uid: a unique identifier for each homicide
- victim_last, victim_first, victim_age, victim_race, victim_sex
- city, state
- lat, lon: geocoded location
- reported_date: when the homicide was reported
- disposition:case outcome
      â€œClosed by arrestâ€
      â€œClosed without arrestâ€
      â€œOpen/No arrestâ€
- other contextual variables describing the case



## Create city_state variable
```{r}
homicide_df = 
  homicide_df |> 
    mutate(
      city_state = paste(city, state, sep = ", ")
    ) 
```

## Description within cities
```{r}
summarize_homicides = function(df, disposition_col = "disposition",
                                unsolved_values = c("Closed without arrest", "Open/No arrest")) {
  df |> 
    summarize(
      total_homicide = n(),
      unsolved_homicide = sum(.data[[disposition_col]] %in% unsolved_values)
    )
}

city_summary =
  homicide_df |>
  group_by(city_state) |>
  summarize_homicides() |>
  ungroup()

city_summary
```



## prop.test on Baltimore, MD
```{r}
baltimore_proptest = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize_homicides() |> 
 (\(df) prop.test(
    x = pull(df, unsolved_homicide),
    n = pull(df, total_homicide)
  ))() |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_proptest
```

## prop.test for each of the cities
```{r}
city_proptest = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize_homicides() |> 
  mutate(
    test_result = map2(
      unsolved_homicide,
      total_homicide, ~ prop.test(.x, .y)  |>  
    broom::tidy())) |> 
  unnest(test_result) |> 
  select(city_state, estimate, conf.low, conf.high)

city_proptest
```


```{r}
city_proptest |> 
    arrange(estimate) |> 
    mutate(
      city_state = factor(city_state, levels = city_state)
      ) |> 
    ggplot(aes(x = city_state, y = estimate)) +
    geom_point(size = 1, color = "purple") +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high),       
                  width = 0.2, color = "purple") +
    labs(
      x = "City",
      y = "Estimated Proportion Unsolved",
      title = "Estimated Proportion of Unsolved Homicides by City",
    ) +
    coord_cartesian(ylim = c(0.20, 0.80)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```



