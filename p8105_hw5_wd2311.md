p8105_hw5_wd2311
================
wd2311
2025-11-14

## Loading package

``` r
library(tidyverse)
```

    ## â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€
    ## âœ” dplyr     1.1.4     âœ” readr     2.1.5
    ## âœ” forcats   1.0.0     âœ” stringr   1.5.2
    ## âœ” ggplot2   4.0.0     âœ” tibble    3.3.0
    ## âœ” lubridate 1.9.4     âœ” tidyr     1.3.1
    ## âœ” purrr     1.1.0     
    ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
    ## âœ– dplyr::filter() masks stats::filter()
    ## âœ– dplyr::lag()    masks stats::lag()
    ## â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(rvest)
```

    ## 
    ## Attaching package: 'rvest'
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

## Figures settings

``` r
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

``` r
set.seed(1)
```

## Define birthday-checking function

``` r
bday_sim = function(n_room) {
  birthdays = sample(1:365, n_room, replace = TRUE)
repeated_bday = length(unique(birthdays)) < n_room
repeated_bday
}

bday_sim(20)
```

    ## [1] FALSE

## 10000 simulation runs

``` r
bday_sim_results = 
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(bdays) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

## Plot the function

``` r
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    x = "Group Size", 
    y = "Probability",
    title = "Probability of Sharing Birthday for Increasing Group Size"
  )
```

<img src="p8105_hw5_wd2311_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

The probability is very small for small groups, but it rises quickly as
group size increases. Around 23 people, the probability is already about
0.5â€”thereâ€™s roughly a 50% chance that at least two people share a
birthday.By about 40â€“50 people, the probability is above 90%,
approaching 1 as the group grows. This is the classic birthday paradox.
Even though there are 365 possible birthdays, we donâ€™t need anything
close to 365 people before a match becomes very likely. The curveâ€™s
steep, nonlinear rise shows how quickly shared birthdays become almost
guaranteed as the group size increases.

## Problem 2

## Set ðœ‡=0 and Generate 5000 datasets from the model

``` r
powermu_zero = 
  expand_grid(
    sample_n = 30,
    sample_sd = 5,
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(data = map(iter, ~rnorm(sample_n, mean = mu, sd = sample_sd))) |> 
  mutate(tidy = map(data, ~broom::tidy(t.test(.x, mu = 0)))) |> 
  mutate(mu_hat = map_dbl(tidy, "estimate"),
         p_value = map_dbl(tidy, "p.value"))
```

## Repeat the above for ðœ‡={1,2,3,4,5,6}

``` r
power_sim = 
  expand_grid(
    mu = 1:6, 
    iter = 1:5000) |> 
  mutate(
    data = map(mu, ~ rnorm(n = 30, mean = .x, sd = 5)),
    ttest = map(data, ~broom::tidy(t.test(.x, mu = 0)))
    
  ) |> 
  unnest(ttest) |> 
  select(mu, iter, estimate, p.value)
```

## Describe the association between effect size and power.

``` r
power_sim |> 
  group_by(mu) |> 
  summarize(
    power = mean(p.value < 0.05)
  ) |> 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
  x = "True Value of Î¼",
  y = "Power (Proportion Null Rejected)",
  title = "Power and the True Value of Î¼"
)
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

<img src="p8105_hw5_wd2311_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

As the true value of Î¼ increases, the power of the one-sample t-test
increases sharply. When Î¼ is near zero, the probability of rejecting the
null hypothesis is near the nominal Î± = 0.05. As Î¼ grows from 1 to 3,
the test becomes increasingly sensitive and power rises rapidly. For Î¼ â‰¥
4, power approaches 1, meaning the test almost always detects the
effect. This demonstrates that larger effect sizes lead to higher power,
given fixed sample size and variance.

## Average Estimates of Î¼ vs.Â True Î¼

``` r
power_sim |> 
  group_by(mu) |> 
  summarize(
    avg_est = mean(estimate),
    avg_est_rejectnull = mean(estimate[p.value < 0.05], na.rm = TRUE)
  ) |> 
  pivot_longer(cols = c(avg_est, avg_est_rejectnull),
               names_to = "type",
               values_to = "value") |> 
  ggplot(aes(x = mu, y = value, color = type)) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(labels = c("All Estimates", "Estimates (Null Rejected)")) +
  labs(
    x = "True Value of Î¼",
    y = "Average Estimate of Î¼",
    color = "Estimate Type",
    title = "Average Estimates of Î¼ vs. True Î¼"
  )
```

<img src="p8105_hw5_wd2311_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

The average Î¼Ì‚ among samples where the null is rejected is not equal to
the true Î¼, especially when Î¼ is small.This is because restricting to
significant tests selects only the samples in which random noise pushed
Î¼Ì‚ upward enough to pass the significance threshold. This creates upward
bias in the estimate, known as the â€œwinnerâ€™s curseâ€ or significance
filter bias. Only when Î¼ is large and power is high do the two averages
converge and become approximately equal.

# Problem 3

## Setting up Data

``` r
homicide_df = read_csv(file = "./data/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## â„¹ Use `spec()` to retrieve the full column specification for this data.
    ## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## Describe raw data

The dataset collected by The Washington Post contains detailed
information on more than 52179 homicide cases from 50 major U.S. cities
between 2007 and 2017. Each row represents a single homicide.

Key Variables: - uid: a unique identifier for each homicide -
victim_last, victim_first, victim_age, victim_race, victim_sex - city,
state - lat, lon: geocoded location - reported_date: when the homicide
was reported - disposition:case outcome â€œClosed by arrestâ€ â€œClosed
without arrestâ€ â€œOpen/No arrestâ€ - other contextual variables describing
the case

## Create city_state variable

``` r
homicide_df = 
  homicide_df |> 
    mutate(
      city_state = paste(city, state, sep = ", ")
    ) 
```

## Description within cities

``` r
summarize_homicides = function(df, disposition_col = "disposition",
                                unsolved_values = c("Closed without arrest", "Open/No arrest")) {
  df |> 
    summarize(
      total_homicide = n(),
      unsolved_homicide = sum(.data[[disposition_col]] %in% unsolved_values)
    )
}

city_summary =
  homicide_df |>
  group_by(city_state) |>
  summarize_homicides() |>
  ungroup()

city_summary
```

    ## # A tibble: 51 Ã— 3
    ##    city_state      total_homicide unsolved_homicide
    ##    <chr>                    <int>             <int>
    ##  1 Albuquerque, NM            378               146
    ##  2 Atlanta, GA                973               373
    ##  3 Baltimore, MD             2827              1825
    ##  4 Baton Rouge, LA            424               196
    ##  5 Birmingham, AL             800               347
    ##  6 Boston, MA                 614               310
    ##  7 Buffalo, NY                521               319
    ##  8 Charlotte, NC              687               206
    ##  9 Chicago, IL               5535              4073
    ## 10 Cincinnati, OH             694               309
    ## # â„¹ 41 more rows

## prop.test on Baltimore, MD

``` r
baltimore_proptest = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize_homicides() |> 
 (\(df) prop.test(
    x = pull(df, unsolved_homicide),
    n = pull(df, total_homicide)
  ))() |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

baltimore_proptest
```

    ## # A tibble: 1 Ã— 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

## prop.test for each of the cities

``` r
city_proptest = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize_homicides() |> 
  mutate(
    test_result = map2(
      unsolved_homicide,
      total_homicide, ~ prop.test(.x, .y)  |>  
    broom::tidy())) |> 
  unnest(test_result) |> 
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## â„¹ In argument: `test_result = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
city_proptest
```

    ## # A tibble: 51 Ã— 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # â„¹ 41 more rows

``` r
city_proptest |> 
    arrange(estimate) |> 
    mutate(
      city_state = factor(city_state, levels = city_state)
      ) |> 
    ggplot(aes(x = city_state, y = estimate)) +
    geom_point(size = 1, color = "purple") +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high),       
                  width = 0.2, color = "purple") +
    labs(
      x = "City",
      y = "Estimated Proportion Unsolved",
      title = "Estimated Proportion of Unsolved Homicides by City",
    ) +
    coord_cartesian(ylim = c(0.20, 0.80)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

<img src="p8105_hw5_wd2311_files/figure-gfm/unnamed-chunk-16-1.png" width="90%" />
